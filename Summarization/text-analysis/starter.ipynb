{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to instll libs, uncomment lib to install\n",
    "#!pip install azure-ai-language-conversations==1.1.0\n",
    "#!pip install azure-ai-textanalytics\n",
    "#!pip install azure.core\n",
    "#!pip install python-dotenv\n",
    "#!pip install openai\n",
    "# restart your kermel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to check your versions, uncomment to check\n",
    "#!pip list | findstr azure.ai.textanalytics\n",
    "#!pip list | findstr azure.ai.language.conversations\n",
    "#!pip list | findstr azure.core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding document summarization\n",
    "\n",
    "The cell below defines variable containg some text to summarize\n",
    "and a path to where 3 text files to test with are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_single_document_text= \"At Microsoft, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic, human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI services, I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition: monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three, there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak, hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\"\n",
    "# This set is already an array\n",
    "a_set_of_documents = [\n",
    "        \"The extractive summarization feature uses natural language processing techniques to locate key sentences in an unstructured text document. These sentences collectively convey the main idea of the document. This feature is provided as an API for developers. \" ,\n",
    "        \"They can use it to build intelligent solutions based on the relevant information extracted to support various use cases. Extractive summarization supports several languages. It is based on pretrained multilingual transformer models, part of our quest for holistic representations. \",\n",
    "        \"It draws its strength from transfer learning across monolingual. It also and harness the shared nature of languages to produce models of improved quality and efficiency. \"\n",
    "    ]\n",
    "a_conversation=[\"Agent: Hello, you're chatting with Rene. How may I help you?\"\n",
    "                \"Customer: Hi, I tried to set up wifi connection for Smart Brew 300 espresso machine, but it didn't work.\"\n",
    "                \"Agent: I’m sorry to hear that. Let’s see what we can do to fix this issue. Could you please try the following steps for me? First, could you push the wifi connection button, hold for 3 seconds, then let me know if the power light is slowly blinking on and off every second?\"\n",
    "                \"Customer: Yes, I pushed the wifi connection button, and now the power light is slowly blinking.\"\n",
    "                \"Agent: Great. Thank you! Now, please check in your Contoso Coffee app. Does it prompt to ask you to connect with the machine?\"\n",
    "                \"Customer: No. Nothing happened.\"\n",
    "                \"Agent: I’m very sorry to hear that. Let me see if there’s another way to fix the issue. Please hold on for a minute.\"]\n",
    "a_multi_document = [\n",
    "       'We went to Contoso Steakhouse located at midtown NYC last week for a dinner party, and we adore the spot! '\n",
    "       'They provide marvelous food and they have a great menu. The chief cook happens to be the owner (I think his name is John Doe) '\n",
    "       'and he is super nice, coming out of the kitchen and greeted us all.'\n",
    "       ,\n",
    "\n",
    "       'We enjoyed very much dining in the place! '\n",
    "       'The Sirloin steak I ordered was tender and juicy, and the place was impeccably clean. You can even pre-order from their '\n",
    "       'online menu at www.contososteakhouse.com, call 312-555-0176 or send email to order@contososteakhouse.com! '\n",
    "       'The only complaint I have is the food didn\\'t come fast enough. Overall I highly recommend it!'\n",
    "   ]\n",
    "\n",
    "My_text_files_path=r\"C:\\Repo\\LearnSummarizationA\\Summarization\\text-analysis\\txt_files\"\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the needed libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.textanalytics  import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.language.conversations import ConversationAnalysisClient\n",
    "from dotenv import load_dotenv \n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "# This example requires environment variables named \"LANGUAGE_KEY\" and \"LANGUAGE_ENDPOINT\"\n",
    "load_dotenv()\n",
    "key = os.environ.get('LANGUAGE_KEY')\n",
    "endpoint = os.environ.get('LANGUAGE_ENDPOINT')\n",
    "open_ai_key = os.environ.get('OPENAI_API_KEY')\n",
    "open_ai_endpoint = os.environ.get('OPEN_AI_ENDPOINT')\n",
    "\n",
    "# for debugging only\n",
    "#print(\"key = \", key)\n",
    "#print(\"endpoint = \", endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create some functions to use across our testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to craete and authenticate a textanalytics client\n",
    "# using your key and endpoint \n",
    "# Returns the authenticated conversation_analysis_client\n",
    "def authenticate_ConversationAnalysisClient_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    conversation_analysis_client = ConversationAnalysisClient(\n",
    "            endpoint=endpoint, \n",
    "            credential=ta_credential)\n",
    "    return conversation_analysis_client\n",
    "\n",
    "# A function to craete and authenticate a ConversationAnalysisClient client\n",
    "# using your key and endpoint \n",
    "# Returns the authenticated TextAnalyticsClient\n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "# A function to create an extractive summary of documennt text\n",
    "# The summary is limited to x number of sentences\n",
    "# Returns the summrized text\n",
    "def sample_extractive_summarization(client, doocuments, max_sentences):\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import (\n",
    "        TextAnalyticsClient,\n",
    "        ExtractiveSummaryAction\n",
    "    ) \n",
    "    \n",
    "    poller = client.begin_analyze_actions(\n",
    "        doocuments,\n",
    "        actions=[\n",
    "            ExtractiveSummaryAction(max_sentence_count=max_sentences, api_version=\"2022-05-15-preview\"),\n",
    "        ], \n",
    "    )\n",
    "    summary_text=\"\"\n",
    "    all_summary_text=[]\n",
    "    extract_summary_results = poller.result()\n",
    "    for results in extract_summary_results:\n",
    "        summary_text=\"\"\n",
    "        for result in results:\n",
    "            if result.kind == \"ExtractiveSummarization\":\n",
    "                summary_text=summary_text+ \" \".join([sentence.text for sentence in result.sentences])\n",
    "            elif result.is_error is True:\n",
    "                print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                    result.error.code, result.error.message\n",
    "                ))\n",
    "        all_summary_text==all_summary_text.append(summary_text)\n",
    "    return all_summary_text\n",
    "\n",
    "\n",
    "# A function to create an abstractive summary of documennt text\n",
    "# The summary is limited to x number of sentences\n",
    "# Returns the summrized text\n",
    "def sample_abstractive_summarization(client, documents, max_sentences):\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import (\n",
    "        TextAnalyticsClient,\n",
    "        AbstractiveSummaryAction\n",
    "    ) \n",
    "    \n",
    "    poller = client.begin_analyze_actions(\n",
    "        documents,\n",
    "        actions=[\n",
    "            AbstractiveSummaryAction(max_sentence_count=max_sentences, api_version=\"2022-05-15-preview\"),\n",
    "        ], \n",
    "    )\n",
    "    summary_text=\"\"\n",
    "    all_summary_text=[]\n",
    "    extract_summary_results = poller.result()\n",
    "    for results in extract_summary_results:\n",
    "        summary_text=\"\"\n",
    "        for result in results:\n",
    "            if result.kind == \"AbstractiveSummarization\":\n",
    "                summary_text=summary_text+ \" \".join([sentence.text for sentence in result.summaries])\n",
    "            elif result.is_error is True:\n",
    "                print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                    result.error.code, result.error.message\n",
    "                ))\n",
    "        all_summary_text==all_summary_text.append(summary_text)\n",
    "    return all_summary_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Extractive Summarization\n",
    "\n",
    "### Now we will use the python SDK (specificallly the TextAnalyticsClient) to perform Extractive Summarization\n",
    "\n",
    "This is an asychronous call\n",
    "\n",
    "Be sure you have environment variables set for the LANGUAGE_KEY and LANGUAGE_ENDPOINT in the .env file\n",
    "\n",
    "#### We will perform 3 scenarios:\n",
    "1. Summarizing a single document to no more than 4 sentences \n",
    "2. Summarizing a set of set of document text as a batch, each to no more than 1 sentence each\n",
    "3. Summarizing each individual text file in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "#\n",
    "# create and authenticate a textanalytics client\n",
    "client = authenticate_client()\n",
    "#\n",
    "# *******************************************************************\n",
    "# Scenario #1 summarize a single document to no more than 4 sentences     \n",
    "# *******************************************************************           \n",
    "document=a_single_document_text\n",
    "#\n",
    "# always pass an array of documents even if just 1 sentence in array\n",
    "documents=[]\n",
    "# append the 1 document to the array\n",
    "documents.append(document)\n",
    "#\n",
    "# limit the number of sentences the summary will contain\n",
    "max_sentences=4\n",
    "#\n",
    "# call the function to summarize the document\n",
    "document_summary = sample_extractive_summarization(client, documents, max_sentences)\n",
    "#\n",
    "print(document_summary)\n",
    "print(\"*************************************\")\n",
    "print(\"sumarizing a single document is done!\")\n",
    "print(\"*************************************\")\n",
    "#\n",
    "# *******************************************************************\n",
    "# summarize a set of set of documents, each to no more than 1 sentence each\n",
    "# *******************************************************************\n",
    "documents= a_set_of_documents\n",
    "#\n",
    "# summarzing to a single sentence\n",
    "max_sentences=1\n",
    "#\n",
    "# call the function to summarize the documents\n",
    "documents_summaries = sample_extractive_summarization(client, documents, max_sentences)\n",
    "\n",
    "# loop through each summary in the returned documents_summaries\n",
    "# and print each one, prepending a label: \"document summary # x\"\n",
    "# where x is the ordinal number in the documents_summaries array\n",
    "for i, summary in enumerate(documents_summaries):\n",
    "    print(\" **** document summary #\", i+1, \": \", summary + \" ****\" + '\\n')\n",
    "\n",
    "#\n",
    "print(\"***************************************\")\n",
    "print (\"Summarizing an array of documents done!\")\n",
    "print(\"***************************************\" +'\\n')\n",
    "print()\n",
    "#\n",
    "# *********************************************\n",
    "# summarize a directory containing multiple txt files\n",
    "# summarize each file in the directory and\n",
    "# write the summary to a new file with the same\n",
    "# name as the original file but with a -summary\n",
    "# at the end of the file name (another .txt file)\n",
    "# Put the summary files in a difernt directory\n",
    "# **********************************************\n",
    "#\n",
    "# get a list of all the files in the directory that have a .txt extension\n",
    "mypath=My_text_files_path + \"\\*.txt\"\n",
    "print(mypath)\n",
    "#\n",
    "# create a list called onlytxtfiles of just the txt files in the direactory\n",
    "onlytxtfiles=glob.glob(mypath)\n",
    "#\n",
    "# print a count of how many txt files are in the collection\n",
    "print(\"There are \", len(onlytxtfiles), \" files in the directory\")\n",
    "#\n",
    "# Then start processing them in a loop\n",
    "print(\"processing files...\" +'\\n')\n",
    "#\n",
    "#loop through the txt files\n",
    "i=0\n",
    "# set the number of sentences to sumarize the text into\n",
    "max_sentences=4\n",
    "#\n",
    "for file in onlytxtfiles:\n",
    "    i=i+1\n",
    "    with open(file, 'r') as myfile:\n",
    "        data=myfile.read().replace('\\n', '')\n",
    "        # always pass an array of documents even if just 1 documents in array\n",
    "        documents=[]\n",
    "        documents.append(data)\n",
    "        # call the function to summarize the document\n",
    "        document_summary = sample_extractive_summarization(client, documents, max_sentences)\n",
    "        # write the summary to a new file with the same name as the original file\n",
    "        # in a different directory\n",
    "        # but with a -summary at the end of the file name\n",
    "        newfile=file.replace(\".txt\", \"-summary.txt\")\n",
    "        newfile=newfile.replace(\"txt_files\",\"ExtractiveSummaries\")\n",
    "        # we are overwriting if they exist!\n",
    "        with open(newfile, 'w') as f:\n",
    "            f.write(str(document_summary[0]))\n",
    "            f.close()\n",
    "            print (\"file written: \", newfile)\n",
    "            print(\" **** file # \", i, \" is done ****\" '\\n')\n",
    "#\n",
    "print(\"***************************************************\")\n",
    "print(\"Done with processing individual files in a directory\" +'\\n')\n",
    "print(\"***************************************************\" +'\\n')\n",
    "\n",
    "print(\"******************************\" + '\\n')\n",
    "print(\"Done with all 3 scenarios!\" + '\\n')\n",
    "print(\"******************************\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Abstractive Summarization\n",
    "\n",
    "### Now we will use the python SDK (specificallly the TextAnalyticsClient) to perform Abstractive Summarization\n",
    "\n",
    "This is an asychronous call\n",
    "\n",
    "Be sure you have environment variables set for the LANGUAGE_KEY and LANGUAGE_ENDPOINT in the .env file\n",
    "\n",
    "#### We will perform just 1 scenario\n",
    "1. Summarizing each individual text file in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********************************************\n",
    "# summarize a directory containing multiple txt files\n",
    "# summarize each file in the directory and\n",
    "# write the summary to a new file with the same\n",
    "# name as the original file but with a -summary\n",
    "# at the end of the file name (another .txt file)\n",
    "# Put the summary files in a difernt directory\n",
    "# **********************************************\n",
    "#\n",
    "# create and authenticate a textanalytics client\n",
    "client = authenticate_client()\n",
    "#\n",
    "# get a list of all the files in the directory that have a .txt extension\n",
    "mypath=My_text_files_path + \"\\*.txt\"\n",
    "print(mypath)\n",
    "#\n",
    "# create a list called onlytxtfiles of just the txt files in the direactory\n",
    "onlytxtfiles=glob.glob(mypath)\n",
    "#\n",
    "# print a count of how many txt files are in the collection\n",
    "print(\"There are \", len(onlytxtfiles), \" files in the directory\")\n",
    "#\n",
    "# Then start processing them in a loop\n",
    "print(\"processing files...\" +'\\n')\n",
    "#\n",
    "#loop through the txt files\n",
    "i=0\n",
    "# set the number of sentences to sumarize the text into\n",
    "max_sentences=4\n",
    "#\n",
    "for file in onlytxtfiles:\n",
    "    i=i+1\n",
    "    with open(file, 'r') as myfile:\n",
    "        data=myfile.read().replace('\\n', '')\n",
    "        # always pass an array of documents even if just 1 documents in array\n",
    "        documents=[]\n",
    "        documents.append(data)\n",
    "        # call the function to summarize the document\n",
    "        document_summary = sample_abstractive_summarization(client, documents, max_sentences)\n",
    "        # write the summary to a new file with the same name as the original file\n",
    "        # in a different directory\n",
    "        # but with a -summary at the end of the file name\n",
    "        newfile=file.replace(\".txt\", \"-summary.txt\")\n",
    "        newfile=newfile.replace(\"txt_files\",\"AbstractiveSummaries\")\n",
    "        # we are overwriting if they exist!\n",
    "        with open(newfile, 'w') as f:\n",
    "            f.write(str(document_summary[0]))\n",
    "            f.close()\n",
    "            print (\"file written: \", newfile)\n",
    "            print(\" **** file # \", i, \" is done ****\" '\\n')\n",
    "print(\"******************************************************************************************\")\n",
    "print(\"Done with generating abstractive summaries for the  individual files in a directory\" +'\\n')\n",
    "print(\"************************************************************************************\" +'\\n')\n",
    "\n",
    "print(\"**************************************************\" + '\\n')\n",
    "print(\"Done with the 1 abstractive summary scenario!\" + '\\n')\n",
    "print(\"*************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using your data - Extractive Summarization\n",
    "\n",
    "### Now use extractive summarization to summarize your call center text files\n",
    "\n",
    "The code assumes they have a .txt extension. Change if necessary.  \n",
    "Also change the max_sentences value as desired.\n",
    "\n",
    "#### We will perform just 1 scenario\n",
    "1. Create an Extractive Summarization each your individual text file in a directory\n",
    "\n",
    "Change the path in the cell below to be where your text files are located.  \n",
    "Also provide a path to where you want the summaries to be written\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************************************************\n",
    "# Change the My_text_files_path variable in the line below to the path to your text files\n",
    "# ***************************************************************************************\n",
    "My_text_files_path=r\"C:\\the local path to your files (with no slash at the end)\"\n",
    "#My_text_files_path=r\"C:\\Repo\\LearnSummarization\\Summarization\\text-analysis\\txt_files\"\n",
    "#\n",
    "# the following cells will make use of the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and authenticate a textanalytics client\n",
    "client = authenticate_client()\n",
    "#\n",
    "# get a list of all the files in the directory that have a .txt extension\n",
    "# change if needed\n",
    "mypath=My_text_files_path + \"\\*.txt\"\n",
    "#print(mypath)\n",
    "#\n",
    "# create a list called onlytxtfiles of just the txt files in the direactory\n",
    "# looking for .txt files, change if needed\n",
    "onlytxtfiles=glob.glob(mypath)\n",
    "\n",
    "# print a count of how many files are in the collection\n",
    "print(\"There are \", len(onlytxtfiles), \" files in the directory\")\n",
    "#\n",
    "print(\"processing files...\" +'\\n')\n",
    "#\n",
    "#loop through the .txt files, change if needed\n",
    "i=0\n",
    "# set the number of sentences to sumarize the text into\n",
    "max_sentences=4\n",
    "#\n",
    "for file in onlytxtfiles:\n",
    "    i=i+1\n",
    "    with open(file, 'r') as myfile:\n",
    "        data=myfile.read().replace('\\n', '')\n",
    "        # always pass an array of documents even if just 1 documents in array\n",
    "        documents=[]\n",
    "        documents.append(data)\n",
    "        # call the function to summarize the document\n",
    "        document_summary = sample_extractive_summarization(client, documents, max_sentences)\n",
    "        # write the summary to a new file with the same name as the original file\n",
    "        # in a different directory\n",
    "        # but with a -summary at the end of the file name\n",
    "        newfile=file.replace(\".txt\", \"-summary.txt\")\n",
    "        newfile=newfile.replace(\"txt_files\",\"ExtractiveSummaries\")\n",
    "        # we are overwriting if they exist!\n",
    "        with open(newfile, 'w') as f:\n",
    "            f.write(str(document_summary[0]))\n",
    "            f.close()\n",
    "            print (\"file written: \", newfile)\n",
    "            print(\" **** file # \", i, \" is done ****\" '\\n')\n",
    "print(\"*********************************************************\")\n",
    "print(\"Done with processing your individual files in a directory\" +'\\n')\n",
    "print(\"*********************************************************\" +'\\n')\n",
    "#\n",
    "print(\"*************************************************************\" + '\\n')\n",
    "print(\"Done with the Extractive Summarization scenario on your data!\" + '\\n')\n",
    "print(\"*************************************************************\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now for an *abstractive* ....**\n",
    "\n",
    "## Using your data - Abstractive Summarization\n",
    "\n",
    "### Now use abstractive summarization to summarize your call center text files\n",
    "\n",
    "The code assumes they have a .txt extension. Change if necessary.  \n",
    "Also change the max_sentences value as desired.\n",
    "\n",
    "#### We will perform just 1 scenario\n",
    "1. Create an Abstractive Summarization each your individual text file in a directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and authenticate a textanalytics client\n",
    "client = authenticate_client()\n",
    "#\n",
    "# *****************************************************************\n",
    "# Change the pathe in the line below to the path to your text files (no slash at end)\n",
    "# *****************************************************************\n",
    "#My_text_files_path=r\"C:\\the local path to your files\"\n",
    "My_text_files_path=r\"C:\\Repo\\LearnSummarization\\Summarization\\text-analysis\\txt_files\"\n",
    "#\n",
    "# get a list of all the files in the directory that have a .txt extension\n",
    "# change if needed\n",
    "\n",
    "mypath=My_text_files_path + \"\\*.txt\"\n",
    "print(mypath)\n",
    "#\n",
    "# create a list called onlytxtfiles of just the txt files in the direactory\n",
    "# looking for .txt files, change if needed\n",
    "onlytxtfiles=glob.glob(mypath)\n",
    "\n",
    "# print a count of how many files are in the collection\n",
    "print(\"There are \", len(onlytxtfiles), \" files in the directory\")\n",
    "print(\"processing files...\" +'\\n')\n",
    "#\n",
    "#loop through the .txt files, change if needed\n",
    "i=0\n",
    "# set the number sentences to sumarize the text into\n",
    "max_sentences=4\n",
    "#\n",
    "for file in onlytxtfiles:\n",
    "    i=i+1\n",
    "    with open(file, 'r') as myfile:\n",
    "        data=myfile.read().replace('\\n', '')\n",
    "        # always pass an array of documents even if just 1 documents in array\n",
    "        documents=[]\n",
    "        documents.append(data)\n",
    "        # call the function to summarize the document\n",
    "        document_summary = sample_abstractive_summarization(client, documents, max_sentences)\n",
    "        # write the summary to a new file with the same name as the original file\n",
    "        # in a different directory\n",
    "        # but with a -summary at the end of the file name\n",
    "        newfile=file.replace(\".txt\", \"-summary.txt\")\n",
    "        newfile=newfile.replace(\"txt_files\",\"AbstractiveSummaries\")\n",
    "        # we are overwriting if they exist!\n",
    "        with open(newfile, 'w') as f:\n",
    "            f.write(str(document_summary[0]))\n",
    "            f.close()\n",
    "            print (\"file written: \", newfile)\n",
    "            print(\" **** file # \", i, \" is done ****\" '\\n')\n",
    "print(\"*********************************************************\")\n",
    "print(\"Done with processing your individual files in a directory\" +'\\n')\n",
    "print(\"*********************************************************\" +'\\n')\n",
    "#\n",
    "print(\"*************************************************************\" + '\\n')\n",
    "print(\"Done with the Extractive Summarization scenario on your data!\" + '\\n')\n",
    "print(\"*************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review your summary files!\n",
    "\n",
    "### Which summarization type is what you want, **Extractive or Abstractive**?\n",
    "\n",
    "### Which did a better summarization,  the **Azure Open Service** *or* the **Language Service**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Open AI\n",
    "### Could it do as good of a job at summarization\n",
    "\n",
    "Let's try! I'll the C:\\Repo\\LearnSummarization\\Summarization\\text-analysis\\txt_files\\sample1.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: The openai-python library support for Azure OpenAI is in preview.\n",
    "import os\n",
    "import openai\n",
    "openai.api_type = \"azure\"\n",
    "# Change the endpoint\n",
    "openai.api_base = \"https://<your endpoint>.openai.azure.com/\"\n",
    "#\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  engine=\"<your depployment name>\",\n",
    "  messages = [{\"role\":\"system\",\"content\":\"You are an AI assistant whos job is to summarize text.You will be giveven some text and your should perform an abstractive summarization. Summarize the text into no more than 4 sentences. Avoid making the sentences too long.\"},\n",
    "              {\"role\":\"user\",\"content\":\"Text to summarize: At Microsoft, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic, human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI services, I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition: monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three, there’s magic—what we call XYZ-code as illustrated in Figure 1—a joint representation to create more powerful AI that can speak, hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pre-trained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multi-sensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\"}],\n",
    "  temperature=0.7,\n",
    "  max_tokens=800,\n",
    "  top_p=0.95,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=None)\n",
    "print(response[\"choices\"][0]['message'][\"content\"]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Topic\n",
    "\n",
    "### But what if you wanted other features of the Language Service like:\n",
    "* Detecting the language\n",
    "* Identifying the key phrases\n",
    "* Identifying Sentiment\n",
    "* Extracting entities\n",
    "* Extracting linked entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_analyze_sentiment() -> None:\n",
    "    print(\n",
    "        \"In this sample we will be combing through reviews customers have left about their\"\n",
    "        \"experience using our skydiving company, Contoso.\"\n",
    "    )\n",
    "    print(\"****************************************************************************************\")\n",
    "    print(\n",
    "        \"We start out with a list of reviews. Let us extract the reviews we are sure are \"\n",
    "        \"positive, so we can display them on our website and get even more customers!\"\n",
    "    )\n",
    "    print(\"****************************************************************************************\")\n",
    "    # [START analyze_sentiment]\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "    # create and authenticate a textanalytics client\n",
    "    text_analytics_client = authenticate_client()\n",
    "\n",
    "    documents = [\n",
    "        \"\"\"I had the best day of my life. I decided to go sky-diving and it made me appreciate my whole life so much more.\n",
    "        I developed a deep-connection with my instructor as well, and I feel as if I've made a life-long friend in her.\"\"\",\n",
    "        \"\"\"This was a waste of my time. All of the views on this drop are extremely boring, all I saw was grass. 0/10 would\n",
    "        not recommend to any divers, even first timers.\"\"\",\n",
    "        \"\"\"This was pretty good! The sights were ok, and I had fun with my instructors! Can't complain too much about my experience\"\"\",\n",
    "        \"\"\"I only have one word for my experience: WOW!!! I can't believe I have had such a wonderful skydiving company right\n",
    "        in my backyard this whole time! I will definitely be a repeat customer, and I want to take my grandmother skydiving too,\n",
    "        I know she'll love it!\"\"\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    result = text_analytics_client.analyze_sentiment(documents, show_opinion_mining=True)\n",
    "    docs = [doc for doc in result if not doc.is_error]\n",
    "    print(\"Let's visualize the sentiment of each of these documents\")\n",
    "    print(\"****************************************************************************************\" + '\\n')\n",
    "    for idx, doc in enumerate(docs):\n",
    "        print('\\t' + f\"Document text: {documents[idx]}\"+ '\\n')\n",
    "        print('\\t' + f\"Overall sentiment: {doc.sentiment}\" + '\\n')\n",
    "        print(\"****************************************************************************************\" )\n",
    "    print(\"Now, let us extract all of the positive reviews\")\n",
    "    positive_reviews = [doc for doc in docs if doc.sentiment == 'positive']\n",
    "    print(\"We want to be very confident that our reviews are positive since we'll be posting them on our website.\")\n",
    "    print(\"We're going to confirm our chosen reviews are positive using two different tests\")\n",
    "    print(\"****************************************************************************************\")\n",
    "    print(\n",
    "        \"**** Test #1 ****\"\n",
    "        \"First, we are going to check how confident the sentiment analysis model is that a document is positive. \"\n",
    "        \"Let's go with a 90% confidence.\"\n",
    "    )\n",
    "    # filter fr only those with a high confidence score\n",
    "    positive_reviews = [\n",
    "        review for review in positive_reviews\n",
    "        if review.confidence_scores.positive >= 0.9\n",
    "    ]\n",
    "    print(\"Done! There are now {} positive reviews with a confidence score of at least 90%\".format(len(positive_reviews)))\n",
    "    print( \n",
    "        \"**** Test #2 ****\"\n",
    "        \"Finally, we also want to make sure every sentence is positive so we only showcase our best selves!\" + '\\n'\n",
    "    )\n",
    "    # we filter the filtered set sentence by sentence\n",
    "    positive_reviews_final = []\n",
    "    for idx, review in enumerate(positive_reviews):\n",
    "        print(f\"Looking at positive review #{idx + 1}\")\n",
    "        any_sentence_not_positive = False\n",
    "        for sentence in review.sentences:\n",
    "            print(\"...Sentence '{}' has sentiment '{}' with confidence scores '{}'\".format(\n",
    "                sentence.text,\n",
    "                sentence.sentiment,\n",
    "                sentence.confidence_scores\n",
    "                )\n",
    "            )\n",
    "            if sentence.sentiment != 'positive':\n",
    "                any_sentence_not_positive = True\n",
    "        if not any_sentence_not_positive:\n",
    "            positive_reviews_final.append(review)\n",
    "    print(\"****************************************************************************************\" +'\\n')\n",
    "    print(\"We now have the final list of positive reviews we are going to display on our website!\" + '\\n')\n",
    "\n",
    "\n",
    "# Main\n",
    "sample_analyze_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Language\n",
    "\n",
    "This sample demonstrates how to detect language in a batch of different documents.\n",
    "\n",
    "In this sample, we own a hotel with a lot of international clientele. We are looking to catalog the reviews we have for our hotel by language, so  we can translate these reviews into English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_detect_language() -> None:\n",
    "    print(\"*************************************************************************************************************\" + '\\n')\n",
    "    print(\n",
    "        \"In this sample we own a hotel with customers from all around the globe. We want to eventually\\n\"\n",
    "        \"translate these reviews into English so our manager can read them. However, we first need to know which language\\n\"\n",
    "        \"they are in for more accurate translation. This is the step we will be covering in this sample\\n\"\n",
    "    )\n",
    "    print(\"*************************************************************************************************************\" + '\\n')\n",
    "    # [START detect_language]\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "    # create and authenticate a textanalytics client\n",
    "    text_analytics_client = authenticate_client()\n",
    "\n",
    "    documents = [\n",
    "        \"\"\"\n",
    "        The concierge Paulette was extremely helpful. Sadly when we arrived the elevator was broken, but with Paulette's help we barely noticed this inconvenience.\n",
    "        She arranged for our baggage to be brought up to our room with no extra charge and gave us a free meal to refurbish all of the calories we lost from\n",
    "        walking up the stairs :). Can't say enough good things about my experience!\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        最近由于工作压力太大，我们决定去富酒店度假。那儿的温泉实在太舒服了，我跟我丈夫都完全恢复了工作前的青春精神！加油！\n",
    "        \"\"\"\n",
    "    ]\n",
    "\n",
    "    result = text_analytics_client.detect_language(documents)\n",
    "    reviewed_docs = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "    print(\"Let's see what language each review is in!\")\n",
    "    print(\"*************************************************************************************************************\" + '\\n')\n",
    "\n",
    "    for idx, doc in enumerate(reviewed_docs):\n",
    "        print(\"\\tReview #{} is in '{}', which has ISO639-1 name '{}'\\n\".format(\n",
    "            idx, doc.primary_language.name, doc.primary_language.iso6391_name\n",
    "        ))\n",
    "        print(\"*************************************************************************************************************\" + '\\n')\n",
    "    # [END detect_language]\n",
    "    print(\n",
    "        \"When actually storing the reviews, we want to map the review to their ISO639-1 name \"\n",
    "        \"so everything is more standardized\"\n",
    "    )\n",
    "\n",
    "    review_to_language = {}\n",
    "    for idx, doc in enumerate(reviewed_docs):\n",
    "        review_to_language[documents[idx]] = doc.primary_language.iso6391_name\n",
    "\n",
    "\n",
    "sample_detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Phrases\n",
    "This sample demonstrates how to extract key talking points from a batch of documents.\n",
    "\n",
    "In this sample, we want to go over articles and read the ones that mention Microsoft.\n",
    "We're going to use the SDK to create a rudimentary search algorithm to find these articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_extract_key_phrases() -> None:\n",
    "    print(\"*************************************************************************************************************\" + '\\n')\n",
    "    print(\n",
    "        \"In this sample, we want to find the articles that mention Microsoft to read.\\n\"\n",
    "    )\n",
    "    print(\"*************************************************************************************************************\" + '\\n')\n",
    "    articles_that_mention_microsoft = []\n",
    "    #\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "    # create and authenticate a textanalytics client\n",
    "    text_analytics_client = authenticate_client()\n",
    "    \n",
    "    articles = [\n",
    "        \"\"\"\n",
    "        Washington, D.C. Autumn in DC is a uniquely beautiful season. The leaves fall from the trees\n",
    "        in a city chock-full of forests, leaving yellow leaves on the ground and a clearer view of the\n",
    "        blue sky above...\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        Redmond, WA. In the past few days, Microsoft has decided to further postpone the start date of\n",
    "        its United States workers, due to the pandemic that rages with no end in sight...\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        Redmond, WA. Employees at Microsoft can be excited about the new coffee shop that will open on campus\n",
    "        once workers no longer have to work remotely...\n",
    "        \"\"\"\n",
    "    ]\n",
    "\n",
    "    result = text_analytics_client.extract_key_phrases(articles)\n",
    "    for idx, doc in enumerate(result):\n",
    "        if not doc.is_error:\n",
    "            print(\"Key phrases in article #{}: {}\".format(\n",
    "                idx + 1,\n",
    "                \", \".join(doc.key_phrases)\n",
    "            ))\n",
    "    # Shows how to filter out the articles that mention Microsoft\n",
    "            if \"Microsoft\" in doc.key_phrases:\n",
    "                articles_that_mention_microsoft.append(str(idx + 1))\n",
    "    print(\"\\n*************************************************************************************************************\" + '\\n')\n",
    "    print(\n",
    "        \"The articles that mention Microsoft are articles number: {}. Those are the ones I'm interested in reading.\".format(\n",
    "            \", \".join(articles_that_mention_microsoft)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "sample_extract_key_phrases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognize PII\n",
    "\n",
    "This sample demonstrates how to recognize personally identifiable information in a batch of documents.\n",
    "The endpoint recognize_pii_entities is only available for API version v3.1 and up.\n",
    "\n",
    "In this sample, we will be working for a company that handles loan payments. To follow privacy guidelines,\n",
    "we need to redact all of our information before we make it public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_recognize_pii_entities() -> None:\n",
    "    print(\"\\n*************************************************************************************************************\" + '\\n')\n",
    "    print(\n",
    "        \"In this sample we will be going through our customer's loan payment information and redacting\\n\"\n",
    "        \"all PII (personally identifiable information) before storing this information on our public website.\\n\"\n",
    "        \"I'm also looking to explicitly extract the SSN information, so I can update my database with SSNs for\\n\"\n",
    "        \"our customers\"\n",
    "    )\n",
    "    print(\"\\n*************************************************************************************************************\" + '\\n')\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "    # create and authenticate a textanalytics client\n",
    "    text_analytics_client = authenticate_client()\n",
    "    \n",
    "    documents = [\n",
    "        \"\"\"Parker Doe has repaid all of their loans as of 2020-04-25.\n",
    "        Their SSN is 859-98-0987. To contact them, use their phone number\n",
    "        555-555-5555. They are originally from Brazil and have Brazilian CPF number 998.214.865-68\"\"\"\n",
    "    ]\n",
    "\n",
    "    result = text_analytics_client.recognize_pii_entities(documents)\n",
    "    docs = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "    print(\n",
    "        \"Let's compare the original document with the documents after redaction. \"\n",
    "        \"I also want to comb through all of the entities that got redacted\"\n",
    "    )\n",
    "    for idx, doc in enumerate(docs):\n",
    "        print(\"\\n*************************************************************************************************************\" + '\\n')\n",
    "        print(f\"Document text: {documents[idx]}\\n\")\n",
    "        print(f\"Redacted document text: {doc.redacted_text}\\n\")\n",
    "        print(\"\\t*************************************************************************************************************\" + '\\n')\n",
    "        for entity in doc.entities:\n",
    "            print(\"\\t...Entity '{}' with category '{}' got redacted\".format(\n",
    "                entity.text, entity.category\n",
    "            ))\n",
    "            print(\"\\n\\t*************************************************************************************************************\" + '\\n')\n",
    "\n",
    "    print(\"All of the information that I expect to be redacted is!\")\n",
    "\n",
    "    print(\n",
    "        \"Now I want to explicitly extract SSN information to add to my user SSN database. \"\n",
    "        \"I also want to be fairly confident that what I'm storing is an SSN, so let's also \"\n",
    "        \"ensure that we're > 60% positive the entity is a SSN\"\n",
    "    )\n",
    "    social_security_numbers = []\n",
    "    for doc in docs:\n",
    "        for entity in doc.entities:\n",
    "            # showing a filter\n",
    "            if entity.category == 'USSocialSecurityNumber' and entity.confidence_score >= 0.6:\n",
    "                social_security_numbers.append(entity.text)\n",
    "    print(\"\\n\\t*************************************************************************************************************\" + '\\n')\n",
    "    print(\"\\tWe have extracted the following SSNs: '{}'\".format(\n",
    "        \"', '\".join(social_security_numbers)\n",
    "    ))\n",
    "    print(\"\\n\\t*************************************************************************************************************\" + '\\n')\n",
    "\n",
    "\n",
    "# Main\n",
    "sample_recognize_pii_entities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Healthcare Entities\n",
    "This sample demonstrates how to detect healthcare entities in a batch of documents.  \n",
    "\n",
    "In this sample we will be a newly-hired engineer working in a pharmacy. We are going to  \n",
    "comb through all of the prescriptions our pharmacy has fulfilled so we can catalog how  \n",
    "much inventory we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_analyze_healthcare_entities() -> None:\n",
    "    import os\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import TextAnalyticsClient, HealthcareEntityRelation\n",
    "    print(\n",
    "        \"In this sample we will be combing through the prescriptions our pharmacy has fulfilled \"\n",
    "        \"so we can catalog how much inventory we have\"\n",
    "    )\n",
    "    print(\n",
    "        \"We start out with a list of prescription documents.\\n\"\n",
    "    )\n",
    "\n",
    "    # create and authenticate a textanalytics client\n",
    "    text_analytics_client = authenticate_client()\n",
    "\n",
    "    documents = [\n",
    "        \"\"\"Patient needs to take 100 mg of ibuprofen, and 3 mg of potassium. Also needs to take 10 mg of Zocor.\"\"\",\n",
    "        \"\"\"Patient needs to take 50 mg of ibuprofen, and 2 mg of Coumadin.\"\"\"\n",
    "    ]\n",
    "    print(documents,)\n",
    "    poller = text_analytics_client.begin_analyze_healthcare_entities(documents)\n",
    "    result = poller.result()\n",
    "\n",
    "    docs = [doc for doc in result if not doc.is_error]\n",
    "    print(\"*************************************************************************************************************\" + '\\n')\n",
    "    print(\"Let's first visualize the outputted healthcare result:\\n\")\n",
    "    print(\"*************************************************************************************************************\" + '\\n')\n",
    "    for doc in docs:\n",
    "        for entity in doc.entities:\n",
    "            print(f\"Entity: {entity.text}\")\n",
    "            print(f\"...Normalized Text: {entity.normalized_text}\")\n",
    "            print(f\"...Category: {entity.category}\")\n",
    "            print(f\"...Subcategory: {entity.subcategory}\")\n",
    "            print(f\"...Offset: {entity.offset}\")\n",
    "            print(f\"...Confidence score: {entity.confidence_score}\")\n",
    "            if entity.data_sources is not None:\n",
    "                print(\"...Data Sources:\")\n",
    "                for data_source in entity.data_sources:\n",
    "                    print(f\"......Entity ID: {data_source.entity_id}\")\n",
    "                    print(f\"......Name: {data_source.name}\")\n",
    "            if entity.assertion is not None:\n",
    "                print(\"...Assertion:\")\n",
    "                print(f\"......Conditionality: {entity.assertion.conditionality}\")\n",
    "                print(f\"......Certainty: {entity.assertion.certainty}\")\n",
    "                print(f\"......Association: {entity.assertion.association}\")\n",
    "        for relation in doc.entity_relations:\n",
    "            print(f\"Relation of type: {relation.relation_type} has the following roles\")\n",
    "            for role in relation.roles:\n",
    "                print(f\"...Role '{role.name}' with entity '{role.entity.text}'\")\n",
    "        print(\"------------------------------------------\")\n",
    "\n",
    "    print(\"Now, let's get all of medication dosage relations from the documents\")\n",
    "    dosage_of_medication_relations = [\n",
    "        entity_relation\n",
    "        for doc in docs\n",
    "        for entity_relation in doc.entity_relations if entity_relation.relation_type == HealthcareEntityRelation.DOSAGE_OF_MEDICATION\n",
    "    ]\n",
    "    # [END analyze_healthcare_entities]\n",
    "\n",
    "    print(\n",
    "        \"\\tDone!\\n\"\n",
    "        \"Now, I will create a dictionary of medication to total dosage. \"\n",
    "        \"I will use a regex to extract the dosage amount. For simplicity sake, I will assume \"\n",
    "        \"all dosages are represented with numbers and have mg unit.\"\n",
    "    )\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "\n",
    "    medication_to_dosage: dict[str, int] = defaultdict(int)\n",
    "\n",
    "    for relation in dosage_of_medication_relations:\n",
    "        # The DosageOfMedication relation should only contain the dosage and medication roles\n",
    "\n",
    "        dosage_role = next(iter(filter(lambda x: x.name == \"Dosage\", relation.roles)))\n",
    "        medication_role = next(iter(filter(lambda x: x.name == \"Medication\", relation.roles)))\n",
    "\n",
    "        try:\n",
    "            dosage_value = int(re.findall(r\"\\d+\", dosage_role.entity.text)[0]) # we find the numbers in the dosage\n",
    "            medication_to_dosage[medication_role.entity.text] += dosage_value\n",
    "        except StopIteration:\n",
    "            # Error handling for if there's no dosage in numbers.\n",
    "            pass\n",
    "\n",
    "    for medication, dosage in medication_to_dosage.items():\n",
    "        print(\"\\tWe have fulfilled '{}' total mg of '{}'\".format(\n",
    "            dosage, medication\n",
    "        ))\n",
    "\n",
    "\n",
    "# Main\n",
    "sample_analyze_healthcare_entities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Entities\n",
    "\n",
    "This sample demonstrates how to recognize custom entities in documents.\n",
    "Recognizing custom entities is also available as an action type through the begin_analyze_actions API.\n",
    "\n",
    "For information on regional support of custom features and how to train a model to\n",
    "recognize custom entities, see https://aka.ms/azsdk/textanalytics/customentityrecognition\n",
    "\n",
    "You need to have trained a custom model before this sample will work.  \n",
    "you can do it using Language Studio  \n",
    "see: https://learn.microsoft.com/en-us/azure/ai-services/language-service/custom-named-entity-recognition/quickstart?pivots=language-studio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_recognize_custom_entities() -> None:\n",
    "    import os\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "    #project_name = os.environ[\"CUSTOM_ENTITIES_PROJECT_NAME\"]\n",
    "    #deployment_name = os.environ[\"CUSTOM_ENTITIES_DEPLOYMENT_NAME\"]\n",
    "    #\n",
    "    # The below values come from Language Studio\n",
    "    project_name = \"<your project name>\"\n",
    "    deployment_name = \"<Your model deployment name>\"\n",
    "    #\n",
    "    path_to_sample_document =  r\"C:\\Repo\\LearnSummarization\\Summarization\\text-analysis\\CustomEntities\\custom_entities_sample.txt\"\n",
    "    print(path_to_sample_document)\n",
    "    #\n",
    "    # create and authenticate a textanalytics client\n",
    "    text_analytics_client = authenticate_client()\n",
    "\n",
    "    with open(path_to_sample_document) as fd:\n",
    "        document = [fd.read()]\n",
    "\n",
    "    poller = text_analytics_client.begin_recognize_custom_entities(\n",
    "        document,\n",
    "        project_name=project_name,\n",
    "        deployment_name=deployment_name\n",
    "    )\n",
    "\n",
    "    document_results = poller.result()\n",
    "    for custom_entities_result in document_results:\n",
    "        if custom_entities_result.kind == \"CustomEntityRecognition\":\n",
    "            for entity in custom_entities_result.entities:\n",
    "                print(\n",
    "                    \"Entity '{}' has category '{}' with confidence score of '{}'\".format(\n",
    "                        entity.text, entity.category, entity.confidence_score\n",
    "                    )\n",
    "                )\n",
    "        elif custom_entities_result.is_error is True:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                custom_entities_result.code, custom_entities_result.message\n",
    "                )\n",
    "            )\n",
    "\n",
    "# Main# How to check your versions\n",
    "sample_recognize_custom_entities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing multiple actions on a single document \n",
    "\n",
    "#### We will perform just 1 scenario but it will do several things\n",
    "1. Perform the following tasks:\n",
    "    Recognize Entities. Recognize LinkedEntities, Recognize PiiEntities, Extract KeyPhrases and Analyze Sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_recognize_custom_entities() -> None:\n",
    "   import os\n",
    "   from azure.core.credentials import AzureKeyCredential\n",
    "   from azure.ai.textanalytics import (\n",
    "    TextAnalyticsClient,\n",
    "    RecognizeEntitiesAction,\n",
    "    RecognizeLinkedEntitiesAction,\n",
    "    RecognizePiiEntitiesAction,\n",
    "    ExtractKeyPhrasesAction,\n",
    "    AnalyzeSentimentAction,\n",
    "    )\n",
    "   endpoint = os.environ[\"LANGUAGE_ENDPOINT\"]\n",
    "   key = os.environ[\"LANGUAGE_KEY\"]\n",
    "   print(endpoint)\n",
    "   print(key)\n",
    "   text_analytics_client = TextAnalyticsClient(\n",
    "       endpoint=endpoint,\n",
    "       credential=AzureKeyCredential(key),\n",
    "       )\n",
    "\n",
    "   documents = [\n",
    "       'We went to Contoso Steakhouse located at midtown NYC last week for a dinner party, and we adore the spot! '\n",
    "       'They provide marvelous food and they have a great menu. The chief cook happens to be the owner (I think his name is John Doe) '\n",
    "       'and he is super nice, coming out of the kitchen and greeted us all.'\n",
    "       ,\n",
    "       \n",
    "       'We enjoyed very much dining in the place! '\n",
    "       'The Sirloin steak I ordered was tender and juicy, and the place was impeccably clean. You can even pre-order from their '\n",
    "       'online menu at www.contososteakhouse.com, call 312-555-0176 or send email to order@contososteakhouse.com! '\n",
    "       'The only complaint I have is the food didn\\'t come fast enough. Overall I highly recommend it!'\n",
    "       ]\n",
    "   poller = text_analytics_client.begin_analyze_actions(\n",
    "       documents,\n",
    "       display_name=\"Sample Text Analysis\",\n",
    "    actions=[\n",
    "        RecognizeEntitiesAction(),\n",
    "        AnalyzeSentimentAction(),\n",
    "        ],\n",
    "    )\n",
    "   document_results = poller.result()\n",
    "   for doc, action_results in zip(documents, document_results):\n",
    "        print(f\"\\nDocument text: {doc}\")\n",
    "        for result in action_results:\n",
    "            if result.kind == \"EntityRecognition\":\n",
    "                print(\"...Results of Recognize Entities Action:\")\n",
    "                for entity in result.entities:\n",
    "                    print(f\"......Entity: {entity.text}\")\n",
    "                    print(f\".........Category: {entity.category}\")\n",
    "                    print(f\".........Confidence Score: {entity.confidence_score}\")\n",
    "                    print(f\".........Offset: {entity.offset}\")\n",
    "            elif result.kind == \"PiiEntityRecognition\":\n",
    "                print(\"...Results of Recognize PII Entities action:\")\n",
    "                for pii_entity in result.entities:\n",
    "                    print(f\"......Entity: {pii_entity.text}\")\n",
    "                    print(f\".........Category: {pii_entity.category}\")\n",
    "                    print(f\".........Confidence Score: {pii_entity.confidence_score}\")\n",
    "            elif result.kind == \"KeyPhraseExtraction\":\n",
    "                print(\"...Results of Extract Key Phrases action:\")\n",
    "                print(f\"......Key Phrases: {result.key_phrases}\")\n",
    "            elif result.kind == \"EntityLinking\":\n",
    "                print(\"...Results of Recognize Linked Entities action:\")\n",
    "                for linked_entity in result.entities:\n",
    "                    print(f\"......Entity name: {linked_entity.name}\")\n",
    "                    print(f\".........Data source: {linked_entity.data_source}\")\n",
    "                    print(f\".........Data source language: {linked_entity.language}\")\n",
    "                    print(\n",
    "                        f\".........Data source entity ID: {linked_entity.data_source_entity_id}\"\n",
    "                        )\n",
    "                    print(f\".........Data source URL: {linked_entity.url}\")\n",
    "                    print(\".........Document matches:\")\n",
    "                    for match in linked_entity.matches:\n",
    "                        print(f\"............Match text: {match.text}\")\n",
    "                        print(f\"............Confidence Score: {match.confidence_score}\")\n",
    "                        print(f\"............Offset: {match.offset}\")\n",
    "                        print(f\"............Length: {match.length}\")\n",
    "            elif result.kind == \"SentimentAnalysis\":\n",
    "                print(\"...Results of Analyze Sentiment action:\")\n",
    "                print(f\"......Overall sentiment: {result.sentiment}\")\n",
    "                print(\n",
    "                    f\"......Scores: positive={result.confidence_scores.positive}; \\\n",
    "                        neutral={result.confidence_scores.neutral}; \\\n",
    "                            negative={result.confidence_scores.negative} \\n\"\n",
    "                            )\n",
    "            elif result.is_error is True:\n",
    "                print(\n",
    "                    f\"...Is an error with code '{result.error.code}' and message '{result.error.message}'\"\n",
    "                    )\n",
    "                print(\"------------------------------------------\")\n",
    "\n",
    "\n",
    "# Main\n",
    "sample_recognize_custom_entities()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
